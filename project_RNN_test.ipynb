{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: Imports and Config (0.1)\n",
    "Set up the required Python libraries and configure global settings (e.g., seed, device)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gensim.downloader as gensim_downloader\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "# âœ… Config\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())  # True if your system is GPU-ready\n",
    "# print(torch.cuda.get_device_name(0))  # Name of your GPU (if available)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: Load and Inspect Dataset (0.2)\n",
    "Load the compressed dataset and inspect its structure, size, and label distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/arxiv100.csv\")\n",
    "\n",
    "# Basic inspection\n",
    "print(df.head())\n",
    "print(df[\"label\"].value_counts())\n",
    "print(f\"Dataset size: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: Train-Dev Split (0.3)\n",
    "Split the dataset into training and development sets using stratified sampling to preserve class balance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split on label\n",
    "train_texts, dev_texts, train_labels, dev_labels = train_test_split(\n",
    "    df[\"abstract\"], df[\"label\"], \n",
    "    test_size=0.2, \n",
    "    stratify=df[\"label\"], \n",
    "    random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: Text Preprocessing (0.4)\n",
    "Clean the abstract text (lowercasing, removing punctuation, etc.) and prepare it for feature extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower() # Converts to lower\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text) # Removes punctuation and special characters\n",
    "    text = re.sub(r\"\\d+\", \"\", text) # Removes numbers\n",
    "    return text\n",
    "\n",
    "train_texts = train_texts.apply(clean_text)\n",
    "dev_texts = dev_texts.apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: Load Pre-trained Word Embeddings (B.1)\n",
    "Load pre-trained embeddings (e.g., GloVe, Word2Vec) and map vocabulary to embedding vectors.\n",
    "https://nlp.stanford.edu/projects/glove/ for Glove download\n",
    "https://fasttext.cc/docs/en/english-vectors.html for fasttext download wiki-news-300d-1M.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(path=\"data/glove.6B.300d.txt\", embedding_dim=300):\n",
    "    \"\"\"\n",
    "    Load GloVe embeddings from file\n",
    "    \"\"\"\n",
    "    print(f\"Loading GloVe embeddings from {path}...\")\n",
    "    embeddings = {}\n",
    "    \n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=\"Loading embeddings\"):\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "                \n",
    "    print(f\"Loaded {len(embeddings)} word embeddings of dimension {embedding_dim}\")\n",
    "    return embeddings\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_embeddings = load_glove_embeddings(\"data/glove.6B.300d.txt\", embedding_dim=300)\n",
    "\n",
    "def load_word2vec_embeddings(embedding_dim=300):\n",
    "    \"\"\"\n",
    "    Load Word2Vec embeddings using gensim\n",
    "    \"\"\"\n",
    "    print(f\"Loading Word2Vec embeddings (dimension: {embedding_dim})...\")\n",
    "    \n",
    "    # Choose the appropriate model based on dimension\n",
    "    if embedding_dim == 300:\n",
    "        model_name = 'word2vec-google-news-300'\n",
    "    else:\n",
    "        raise ValueError(f\"Word2Vec is only available in 300d. Got {embedding_dim}d\")\n",
    "    \n",
    "    # Load pre-trained model\n",
    "    try:\n",
    "        wv_model = gensim_downloader.load(model_name)\n",
    "        print(f\"Loaded {len(wv_model.key_to_index)} word vectors\")\n",
    "        \n",
    "        # Convert to dictionary for consistency with other embedding functions\n",
    "        embeddings = {word: wv_model.get_vector(word) for word in tqdm(wv_model.key_to_index, desc=\"Processing vectors\")}\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Word2Vec: {e}\")\n",
    "        print(\"Falling back to random embeddings\")\n",
    "        return {}\n",
    "\n",
    "# Load Word2vec embeddings\n",
    "word2vec_embeddings = load_word2vec_embeddings()\n",
    "\n",
    "\n",
    "def load_fasttext_embeddings(path=\"data/wiki-news-300d-1M.vec\", embedding_dim=300):\n",
    "    \"\"\"\n",
    "    Load FastText embeddings from file\n",
    "    \"\"\"\n",
    "    print(f\"Loading FastText embeddings from {path}...\")\n",
    "    embeddings = {}\n",
    "    \n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"FastText embeddings file not found at {path}\")\n",
    "            print(\"Please download from https://fasttext.cc/docs/en/crawl-vectors.html\")\n",
    "            print(\"Falling back to random embeddings\")\n",
    "            return embeddings\n",
    "        \n",
    "        # Load first line to get dimension info\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            first_line = f.readline().strip().split()\n",
    "            vocab_size, dim = int(first_line[0]), int(first_line[1])\n",
    "            \n",
    "            if dim != embedding_dim:\n",
    "                print(f\"Warning: FastText file has {dim}d vectors, but {embedding_dim}d was requested\")\n",
    "            \n",
    "            # Load the vectors (limit to 1M words for memory efficiency)\n",
    "            count = 0\n",
    "            max_words = 1000000\n",
    "            \n",
    "            for line in tqdm(f, desc=\"Loading embeddings\", total=min(vocab_size, max_words)):\n",
    "                if count >= max_words:\n",
    "                    break\n",
    "                    \n",
    "                tokens = line.strip().split()\n",
    "                word = tokens[0]\n",
    "                vector = np.asarray(tokens[1:], dtype='float32')\n",
    "                embeddings[word] = vector\n",
    "                count += 1\n",
    "                \n",
    "        print(f\"Loaded {len(embeddings)} word embeddings of dimension {dim}\")\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading FastText: {e}\")\n",
    "        print(\"Falling back to random embeddings\")\n",
    "        return {}\n",
    "    \n",
    "# Load fasttext embedding\n",
    "fasttext_embedding = load_fasttext_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: Text-to-Sequence Pipeline (B.2)\n",
    "Convert preprocessed abstracts into padded sequences of word indices aligned with the embedding matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, texts=None, max_vocab=20000, min_freq=2):\n",
    "        self.word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.idx2word = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
    "        \n",
    "        if texts:\n",
    "            self.fit(texts, max_vocab, min_freq)\n",
    "    \n",
    "    def fit(self, texts, max_vocab=20000, min_freq=2):\n",
    "        # Count word frequencies\n",
    "        word_counts = Counter()\n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            word_counts.update(words)\n",
    "        \n",
    "        # Keep only words that appear at least min_freq times\n",
    "        vocab = [word for word, count in word_counts.most_common(max_vocab) \n",
    "                if count >= min_freq]\n",
    "        \n",
    "        # Create word-to-index mapping\n",
    "        for word in vocab:\n",
    "            idx = len(self.word2idx)\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "            \n",
    "        print(f\"Vocabulary size: {len(self.word2idx)}\")\n",
    "        \n",
    "    def texts_to_sequences(self, texts):\n",
    "        \"\"\"Convert texts to sequences of word indices\"\"\"\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            seq = [self.word2idx.get(word, 1) for word in words]  # 1 is <UNK>\n",
    "            sequences.append(seq)\n",
    "        return sequences\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "def create_sequence_datasets(train_texts, dev_texts, train_labels, dev_labels, max_vocab=20000, max_len=128):\n",
    "    \"\"\"\n",
    "    Convert texts to padded sequences of word indices\n",
    "    \"\"\"\n",
    "    # Create and fit tokenizer\n",
    "    tokenizer = SimpleTokenizer(train_texts, max_vocab=max_vocab)\n",
    "    \n",
    "    # Convert texts to sequences\n",
    "    train_seqs = tokenizer.texts_to_sequences(train_texts)\n",
    "    dev_seqs = tokenizer.texts_to_sequences(dev_texts)\n",
    "    \n",
    "    # Compute sequence length statistics\n",
    "    train_lengths = [len(seq) for seq in train_seqs]\n",
    "    avg_len = sum(train_lengths) / len(train_lengths)\n",
    "    max_observed = max(train_lengths)\n",
    "    \n",
    "    # Print sequence length stats\n",
    "    print(f\"Average sequence length: {avg_len:.1f}\")\n",
    "    print(f\"Maximum sequence length: {max_observed}\")\n",
    "    print(f\"Using max_len = {max_len}\")\n",
    "    \n",
    "    # Truncate sequences if they're too long\n",
    "    train_seqs = [seq[:max_len] for seq in train_seqs]\n",
    "    dev_seqs = [seq[:max_len] for seq in dev_seqs]\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = [torch.tensor(seq, dtype=torch.long) for seq in train_seqs]\n",
    "    X_dev = [torch.tensor(seq, dtype=torch.long) for seq in dev_seqs]\n",
    "    \n",
    "    # Pad sequences\n",
    "    X_train_pad = pad_sequence(X_train, batch_first=True, padding_value=0)\n",
    "    X_dev_pad = pad_sequence(X_dev, batch_first=True, padding_value=0)\n",
    "    \n",
    "    y_train_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "    y_dev_tensor = torch.tensor(dev_labels, dtype=torch.long)\n",
    "    \n",
    "    print(f\"Training tensor shape: {X_train_pad.shape}\")\n",
    "    print(f\"Dev tensor shape: {X_dev_pad.shape}\")\n",
    "    \n",
    "    return {\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"X_train\": X_train_pad,\n",
    "        \"y_train\": y_train_tensor,\n",
    "        \"X_dev\": X_dev_pad,\n",
    "        \"y_dev\": y_dev_tensor,\n",
    "        \"max_len\": max_len\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: RNN Model Definition (B.3)\n",
    "Define the RNN model architecture using PyTorch (Simple RNN, LSTM, or GRU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 embeddings_matrix=None, rnn_type=\"lstm\", bidirectional=True, \n",
    "                 n_layers=1, dropout=0.5, freeze_embeddings=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Setup embedding layer\n",
    "        self.freeze_embeddings = freeze_embeddings\n",
    "        \n",
    "        if embeddings_matrix is not None:\n",
    "            # Use pre-trained embeddings\n",
    "            self.embedding = nn.Embedding.from_pretrained(\n",
    "                torch.FloatTensor(embeddings_matrix),\n",
    "                padding_idx=0,\n",
    "                freeze=freeze_embeddings  # Now controlled by parameter\n",
    "            )\n",
    "        else:\n",
    "            # Initialize random embeddings\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Select RNN type\n",
    "        if rnn_type == \"lstm\":\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, \n",
    "                              num_layers=n_layers,\n",
    "                              bidirectional=bidirectional, \n",
    "                              dropout=dropout if n_layers > 1 else 0,\n",
    "                              batch_first=True)\n",
    "        elif rnn_type == \"gru\":\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, \n",
    "                             num_layers=n_layers,\n",
    "                             bidirectional=bidirectional, \n",
    "                             dropout=dropout if n_layers > 1 else 0,\n",
    "                             batch_first=True)\n",
    "        else:  # Simple RNN\n",
    "            self.rnn = nn.RNN(embedding_dim, hidden_dim, \n",
    "                             num_layers=n_layers,\n",
    "                             bidirectional=bidirectional, \n",
    "                             dropout=dropout if n_layers > 1 else 0,\n",
    "                             batch_first=True)\n",
    "            \n",
    "        # Define output dimensions based on bidirectionality\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Save for forward pass\n",
    "        self.bidirectional = bidirectional\n",
    "        self.rnn_type = rnn_type\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len]\n",
    "        \n",
    "        # Pass through embedding layer\n",
    "        # Only apply dropout if embeddings are trainable\n",
    "        if self.freeze_embeddings:\n",
    "            embedded = self.embedding(x)\n",
    "        else:\n",
    "            embedded = self.dropout(self.embedding(x))\n",
    "        # embedded shape: [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        if self.rnn_type == \"lstm\":\n",
    "            # Get all outputs and hidden/cell states\n",
    "            output, (hidden, cell) = self.rnn(embedded)\n",
    "        else:\n",
    "            # Get all outputs and hidden state\n",
    "            output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        # Use the last hidden state from all layers\n",
    "        if self.bidirectional:\n",
    "            # Concatenate the final forward and backward hidden states\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        else:\n",
    "            # Just take the final hidden state\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "        \n",
    "        # Pass through linear layer for classification\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: Create Embedding Matrix (B.4)\n",
    "This function will map your tokenizer vocabulary to the pre-trained embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified create_embedding_matrix to handle sparse embeddings\n",
    "def create_embedding_matrix(tokenizer, word_embeddings, embedding_dim):\n",
    "    \"\"\"\n",
    "    Create an embedding matrix for the vocabulary in tokenizer\n",
    "    using pre-trained word embeddings\n",
    "    \"\"\"\n",
    "    vocab_size = tokenizer.get_vocab_size()\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    \n",
    "    # Initialize with random vectors for better training\n",
    "    # Uses a small range to avoid extreme initial values\n",
    "    embedding_matrix = np.random.uniform(-0.1, 0.1, (vocab_size, embedding_dim))\n",
    "    \n",
    "    # Count words found in embeddings\n",
    "    found = 0\n",
    "    \n",
    "    # Fill embedding matrix with pre-trained embeddings\n",
    "    for word, idx in tokenizer.word2idx.items():\n",
    "        if word in word_embeddings:\n",
    "            embedding_matrix[idx] = word_embeddings[word]\n",
    "            found += 1\n",
    "    \n",
    "    print(f\"Found embeddings for {found}/{vocab_size-2} words ({found/(vocab_size-2)*100:.2f}%)\")\n",
    "    return embedding_matrix\n",
    "\n",
    "# Convert Pandas Series to lists first\n",
    "train_texts_list = train_texts.tolist()\n",
    "dev_texts_list = dev_texts.tolist()\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(train_labels)\n",
    "y_dev_encoded = label_encoder.transform(dev_labels)\n",
    "\n",
    "# Create sequence datasets - now passing lists instead of Series\n",
    "seq_data = create_sequence_datasets(\n",
    "    train_texts_list,  \n",
    "    dev_texts_list,    \n",
    "    y_train_encoded, \n",
    "    y_dev_encoded,\n",
    "    max_vocab=20000,\n",
    "    max_len=128\n",
    ")\n",
    "\n",
    "# Create embedding matrix\n",
    "embedding_matrix = create_embedding_matrix(\n",
    "    seq_data[\"tokenizer\"], \n",
    "    glove_embeddings, \n",
    "    embedding_dim=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch DataLoaders for efficient batch processing:\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = TensorDataset(seq_data[\"X_train\"], seq_data[\"y_train\"])\n",
    "dev_dataset = TensorDataset(seq_data[\"X_dev\"], seq_data[\"y_dev\"])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: RNN Training Loop (B.5)\n",
    "Evaluate the RNN using dev data and compute relevant classification metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(model, train_loader, dev_loader, epochs=10, lr=0.001):\n",
    "    \"\"\"Train RNN model with early stopping based on dev set performance\"\"\"\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # For early stopping\n",
    "    best_valid_loss = float('inf')\n",
    "    best_valid_acc = 0\n",
    "    patience = 3\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(f\"Training for {epochs} epochs...\")\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "        # Calculate average loss and accuracy\n",
    "        train_loss = train_loss / train_total\n",
    "        train_acc = train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        valid_correct = 0\n",
    "        valid_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dev_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                valid_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                valid_total += labels.size(0)\n",
    "                valid_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        valid_loss = valid_loss / valid_total\n",
    "        valid_acc = valid_correct / valid_total\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc:.4f}\")\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if valid_acc > best_valid_acc:\n",
    "            best_valid_acc = valid_acc\n",
    "            best_valid_loss = valid_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, best_valid_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: RNN Evaluation (B.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rnn(model, data_loader, label_encoder, title=\"Confusion Matrix\"):\n",
    "    \"\"\"Evaluate RNN model performance\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "    precision = precision_score(all_labels, all_preds, average=\"macro\")\n",
    "    recall = recall_score(all_labels, all_preds, average=\"macro\")\n",
    "    \n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    \n",
    "    # Full classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=label_encoder.classes_,\n",
    "                yticklabels=label_encoder.classes_)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM model\n",
    "lstm_model = RNNClassifier(\n",
    "    vocab_size=seq_data[\"tokenizer\"].get_vocab_size(),\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=256,\n",
    "    output_dim=len(label_encoder.classes_),\n",
    "    embeddings_matrix=embedding_matrix,\n",
    "    rnn_type=\"lstm\",\n",
    "    bidirectional=True,\n",
    "    n_layers=2,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "# Train LSTM\n",
    "lstm_model, lstm_best_acc = train_rnn(\n",
    "    lstm_model, \n",
    "    train_loader, \n",
    "    dev_loader, \n",
    "    epochs=15,\n",
    "    lr=0.001\n",
    ")\n",
    "\n",
    "# Evaluate LSTM\n",
    "lstm_metrics = evaluate_rnn(\n",
    "    lstm_model, \n",
    "    dev_loader, \n",
    "    label_encoder,\n",
    "    title=\"LSTM Performance\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GRU model\n",
    "gru_model = RNNClassifier(\n",
    "    vocab_size=seq_data[\"tokenizer\"].get_vocab_size(),\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=256,\n",
    "    output_dim=len(label_encoder.classes_),\n",
    "    embeddings_matrix=embedding_matrix,\n",
    "    rnn_type=\"gru\",\n",
    "    bidirectional=True,\n",
    "    n_layers=1,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "# Train GRU\n",
    "gru_model, gru_best_acc = train_rnn(\n",
    "    gru_model, \n",
    "    train_loader, \n",
    "    dev_loader, \n",
    "    epochs=15,\n",
    "    lr=0.001\n",
    ")\n",
    "\n",
    "# Evaluate GRU\n",
    "gru_metrics = evaluate_rnn(\n",
    "    gru_model, \n",
    "    dev_loader, \n",
    "    label_encoder,\n",
    "    title=\"GRU Performance\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Simple RNN model\n",
    "simple_rnn_model = RNNClassifier(\n",
    "    vocab_size=seq_data[\"tokenizer\"].get_vocab_size(),\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=256,\n",
    "    output_dim=len(label_encoder.classes_),\n",
    "    embeddings_matrix=embedding_matrix,\n",
    "    rnn_type=\"rnn\",\n",
    "    bidirectional=False,\n",
    "    n_layers=1,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "# Train Simple RNN\n",
    "simple_rnn_model, simple_rnn_best_acc = train_rnn(\n",
    "    simple_rnn_model, \n",
    "    train_loader, \n",
    "    dev_loader, \n",
    "    epochs=15,\n",
    "    lr=0.001\n",
    ")\n",
    "\n",
    "# Evaluate Simple RNN\n",
    "simple_rnn_metrics = evaluate_rnn(\n",
    "    simple_rnn_model, \n",
    "    dev_loader, \n",
    "    label_encoder,\n",
    "    title=\"Simple RNN Performance\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "rnn_results = pd.DataFrame({\n",
    "    \"Model\": [\"LSTM\", \"GRU\", \"Simple RNN\"],\n",
    "    \"Accuracy\": [lstm_metrics[\"accuracy\"], gru_metrics[\"accuracy\"], simple_rnn_metrics[\"accuracy\"]],\n",
    "    \"F1-Score\": [lstm_metrics[\"f1\"], gru_metrics[\"f1\"], simple_rnn_metrics[\"f1\"]],\n",
    "    \"Precision\": [lstm_metrics[\"precision\"], gru_metrics[\"precision\"], simple_rnn_metrics[\"precision\"]],\n",
    "    \"Recall\": [lstm_metrics[\"recall\"], gru_metrics[\"recall\"], simple_rnn_metrics[\"recall\"]]\n",
    "})\n",
    "\n",
    "# Display results\n",
    "print(rnn_results.sort_values(by=\"F1-Score\", ascending=False))\n",
    "\n",
    "# Save the best model\n",
    "torch.save(lstm_model.state_dict(), \"models/best_rnn_model.pt\")\n",
    "print(\"Best model saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
