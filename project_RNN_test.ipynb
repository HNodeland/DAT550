{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: Imports and Config (0.1)\n",
    "Set up the required Python libraries and configure global settings (e.g., seed, device)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gensim.downloader as gensim_downloader\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "# âœ… Config\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())  # True if your system is GPU-ready\n",
    "# print(torch.cuda.get_device_name(0))  # Name of your GPU (if available)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: Load and Inspect Dataset (0.2)\n",
    "Load the compressed dataset and inspect its structure, size, and label distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/arxiv100.csv\")\n",
    "\n",
    "# Basic inspection\n",
    "print(df.head())\n",
    "print(df[\"label\"].value_counts())\n",
    "print(f\"Dataset size: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: Train-Dev Split (0.3)\n",
    "Split the dataset into training and development sets using stratified sampling to preserve class balance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split on label\n",
    "train_texts, dev_texts, train_labels, dev_labels = train_test_split(\n",
    "    df[\"abstract\"], df[\"label\"], \n",
    "    test_size=0.2, \n",
    "    stratify=df[\"label\"], \n",
    "    random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: Text Preprocessing (0.4)\n",
    "Clean the abstract text (lowercasing, removing punctuation, etc.) and prepare it for feature extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower() # Converts to lower\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text) # Removes punctuation and special characters\n",
    "    text = re.sub(r\"\\d+\", \"\", text) # Removes numbers\n",
    "    return text\n",
    "\n",
    "train_texts = train_texts.apply(clean_text)\n",
    "dev_texts = dev_texts.apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: Load Pre-trained Word Embeddings (B.1)\n",
    "Load pre-trained embeddings (e.g., GloVe, Word2Vec) and map vocabulary to embedding vectors.\n",
    "https://nlp.stanford.edu/projects/glove/ for Glove download\n",
    "https://fasttext.cc/docs/en/english-vectors.html for fasttext download wiki-news-300d-1M.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(path=\"data/glove.6B.300d.txt\", embedding_dim=300):\n",
    "    \"\"\"\n",
    "    Load GloVe embeddings from file\n",
    "    \"\"\"\n",
    "    print(f\"Loading GloVe embeddings from {path}...\")\n",
    "    embeddings = {}\n",
    "    \n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=\"Loading embeddings\"):\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "                \n",
    "    print(f\"Loaded {len(embeddings)} word embeddings of dimension {embedding_dim}\")\n",
    "    return embeddings\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_embeddings = load_glove_embeddings(\"data/glove.6B.300d.txt\", embedding_dim=300)\n",
    "\n",
    "def load_word2vec_embeddings(embedding_dim=300):\n",
    "    \"\"\"\n",
    "    Load Word2Vec embeddings using gensim\n",
    "    \"\"\"\n",
    "    print(f\"Loading Word2Vec embeddings (dimension: {embedding_dim})...\")\n",
    "    \n",
    "    # Choose the appropriate model based on dimension\n",
    "    if embedding_dim == 300:\n",
    "        model_name = 'word2vec-google-news-300'\n",
    "    else:\n",
    "        raise ValueError(f\"Word2Vec is only available in 300d. Got {embedding_dim}d\")\n",
    "    \n",
    "    # Load pre-trained model\n",
    "    try:\n",
    "        wv_model = gensim_downloader.load(model_name)\n",
    "        print(f\"Loaded {len(wv_model.key_to_index)} word vectors\")\n",
    "        \n",
    "        # Convert to dictionary for consistency with other embedding functions\n",
    "        embeddings = {word: wv_model.get_vector(word) for word in tqdm(wv_model.key_to_index, desc=\"Processing vectors\")}\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Word2Vec: {e}\")\n",
    "        print(\"Falling back to random embeddings\")\n",
    "        return {}\n",
    "\n",
    "# Load Word2vec embeddings\n",
    "word2vec_embeddings = load_word2vec_embeddings()\n",
    "\n",
    "\n",
    "def load_fasttext_embeddings(path=\"data/wiki-news-300d-1M.vec\", embedding_dim=300):\n",
    "    \"\"\"\n",
    "    Load FastText embeddings from file\n",
    "    \"\"\"\n",
    "    print(f\"Loading FastText embeddings from {path}...\")\n",
    "    embeddings = {}\n",
    "    \n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"FastText embeddings file not found at {path}\")\n",
    "            print(\"Please download from https://fasttext.cc/docs/en/crawl-vectors.html\")\n",
    "            print(\"Falling back to random embeddings\")\n",
    "            return embeddings\n",
    "        \n",
    "        # Load first line to get dimension info\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            first_line = f.readline().strip().split()\n",
    "            vocab_size, dim = int(first_line[0]), int(first_line[1])\n",
    "            \n",
    "            if dim != embedding_dim:\n",
    "                print(f\"Warning: FastText file has {dim}d vectors, but {embedding_dim}d was requested\")\n",
    "            \n",
    "            # Load the vectors (limit to 1M words for memory efficiency)\n",
    "            count = 0\n",
    "            max_words = 1000000\n",
    "            \n",
    "            for line in tqdm(f, desc=\"Loading embeddings\", total=min(vocab_size, max_words)):\n",
    "                if count >= max_words:\n",
    "                    break\n",
    "                    \n",
    "                tokens = line.strip().split()\n",
    "                word = tokens[0]\n",
    "                vector = np.asarray(tokens[1:], dtype='float32')\n",
    "                embeddings[word] = vector\n",
    "                count += 1\n",
    "                \n",
    "        print(f\"Loaded {len(embeddings)} word embeddings of dimension {dim}\")\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading FastText: {e}\")\n",
    "        print(\"Falling back to random embeddings\")\n",
    "        return {}\n",
    "    \n",
    "# Load fasttext embedding\n",
    "fasttext_embedding = load_fasttext_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: Text-to-Sequence Pipeline (B.2)\n",
    "Convert preprocessed abstracts into padded sequences of word indices aligned with the embedding matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, texts=None, max_vocab=20000, min_freq=2):\n",
    "        self.word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.idx2word = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
    "        \n",
    "        if texts:\n",
    "            self.fit(texts, max_vocab, min_freq)\n",
    "    \n",
    "    def fit(self, texts, max_vocab=20000, min_freq=2):\n",
    "        # Count word frequencies\n",
    "        word_counts = Counter()\n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            word_counts.update(words)\n",
    "        \n",
    "        # Keep only words that appear at least min_freq times\n",
    "        vocab = [word for word, count in word_counts.most_common(max_vocab) \n",
    "                if count >= min_freq]\n",
    "        \n",
    "        # Create word-to-index mapping\n",
    "        for word in vocab:\n",
    "            idx = len(self.word2idx)\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "            \n",
    "        print(f\"Vocabulary size: {len(self.word2idx)}\")\n",
    "        \n",
    "    def texts_to_sequences(self, texts):\n",
    "        \"\"\"Convert texts to sequences of word indices\"\"\"\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            seq = [self.word2idx.get(word, 1) for word in words]  # 1 is <UNK>\n",
    "            sequences.append(seq)\n",
    "        return sequences\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "def create_sequence_datasets(train_texts, dev_texts, train_labels, dev_labels, max_vocab=20000, max_len=128):\n",
    "    \"\"\"\n",
    "    Convert texts to padded sequences of word indices\n",
    "    \"\"\"\n",
    "    # Create and fit tokenizer\n",
    "    tokenizer = SimpleTokenizer(train_texts, max_vocab=max_vocab)\n",
    "    \n",
    "    # Convert texts to sequences\n",
    "    train_seqs = tokenizer.texts_to_sequences(train_texts)\n",
    "    dev_seqs = tokenizer.texts_to_sequences(dev_texts)\n",
    "    \n",
    "    # Compute sequence length statistics\n",
    "    train_lengths = [len(seq) for seq in train_seqs]\n",
    "    avg_len = sum(train_lengths) / len(train_lengths)\n",
    "    max_observed = max(train_lengths)\n",
    "    \n",
    "    # Print sequence length stats\n",
    "    print(f\"Average sequence length: {avg_len:.1f}\")\n",
    "    print(f\"Maximum sequence length: {max_observed}\")\n",
    "    print(f\"Using max_len = {max_len}\")\n",
    "    \n",
    "    # Truncate sequences if they're too long\n",
    "    train_seqs = [seq[:max_len] for seq in train_seqs]\n",
    "    dev_seqs = [seq[:max_len] for seq in dev_seqs]\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = [torch.tensor(seq, dtype=torch.long) for seq in train_seqs]\n",
    "    X_dev = [torch.tensor(seq, dtype=torch.long) for seq in dev_seqs]\n",
    "    \n",
    "    # Pad sequences\n",
    "    X_train_pad = pad_sequence(X_train, batch_first=True, padding_value=0)\n",
    "    X_dev_pad = pad_sequence(X_dev, batch_first=True, padding_value=0)\n",
    "    \n",
    "    y_train_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "    y_dev_tensor = torch.tensor(dev_labels, dtype=torch.long)\n",
    "    \n",
    "    print(f\"Training tensor shape: {X_train_pad.shape}\")\n",
    "    print(f\"Dev tensor shape: {X_dev_pad.shape}\")\n",
    "    \n",
    "    return {\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"X_train\": X_train_pad,\n",
    "        \"y_train\": y_train_tensor,\n",
    "        \"X_dev\": X_dev_pad,\n",
    "        \"y_dev\": y_dev_tensor,\n",
    "        \"max_len\": max_len\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: RNN Model Definition (B.3)\n",
    "Define the RNN model architecture using PyTorch (Simple RNN, LSTM, or GRU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 embeddings_matrix=None, rnn_type=\"lstm\", bidirectional=True, \n",
    "                 n_layers=1, dropout=0.5, freeze_embeddings=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Setup embedding layer\n",
    "        self.freeze_embeddings = freeze_embeddings\n",
    "        \n",
    "        if embeddings_matrix is not None:\n",
    "            # Use pre-trained embeddings\n",
    "            self.embedding = nn.Embedding.from_pretrained(\n",
    "                torch.FloatTensor(embeddings_matrix),\n",
    "                padding_idx=0,\n",
    "                freeze=freeze_embeddings  # Now controlled by parameter\n",
    "            )\n",
    "        else:\n",
    "            # Initialize random embeddings\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Select RNN type\n",
    "        if rnn_type == \"lstm\":\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, \n",
    "                              num_layers=n_layers,\n",
    "                              bidirectional=bidirectional, \n",
    "                              dropout=dropout if n_layers > 1 else 0,\n",
    "                              batch_first=True)\n",
    "        elif rnn_type == \"gru\":\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, \n",
    "                             num_layers=n_layers,\n",
    "                             bidirectional=bidirectional, \n",
    "                             dropout=dropout if n_layers > 1 else 0,\n",
    "                             batch_first=True)\n",
    "        else:  # Simple RNN\n",
    "            self.rnn = nn.RNN(embedding_dim, hidden_dim, \n",
    "                             num_layers=n_layers,\n",
    "                             bidirectional=bidirectional, \n",
    "                             dropout=dropout if n_layers > 1 else 0,\n",
    "                             batch_first=True)\n",
    "            \n",
    "        # Define output dimensions based on bidirectionality\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Save for forward pass\n",
    "        self.bidirectional = bidirectional\n",
    "        self.rnn_type = rnn_type\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len]\n",
    "        \n",
    "        # Pass through embedding layer\n",
    "        # Only apply dropout if embeddings are trainable\n",
    "        if self.freeze_embeddings:\n",
    "            embedded = self.embedding(x)\n",
    "        else:\n",
    "            embedded = self.dropout(self.embedding(x))\n",
    "        # embedded shape: [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        if self.rnn_type == \"lstm\":\n",
    "            # Get all outputs and hidden/cell states\n",
    "            output, (hidden, cell) = self.rnn(embedded)\n",
    "        else:\n",
    "            # Get all outputs and hidden state\n",
    "            output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        # Use the last hidden state from all layers\n",
    "        if self.bidirectional:\n",
    "            # Concatenate the final forward and backward hidden states\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        else:\n",
    "            # Just take the final hidden state\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "        \n",
    "        # Pass through linear layer for classification\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: Create Embedding Matrix (B.4)\n",
    "This function will map your tokenizer vocabulary to the pre-trained embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create_embedding_matrix to handle sparse embeddings\n",
    "def create_embedding_matrix(tokenizer, word_embeddings, embedding_dim):\n",
    "    \"\"\"\n",
    "    Create an embedding matrix for the vocabulary in tokenizer\n",
    "    using pre-trained word embeddings\n",
    "    \"\"\"\n",
    "    vocab_size = tokenizer.get_vocab_size()\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    \n",
    "    # Initialize with random vectors for better training\n",
    "    # Uses a small range to avoid extreme initial values\n",
    "    embedding_matrix = np.random.uniform(-0.1, 0.1, (vocab_size, embedding_dim))\n",
    "    \n",
    "    # Count words found in embeddings\n",
    "    found = 0\n",
    "    \n",
    "    # Fill embedding matrix with pre-trained embeddings\n",
    "    for word, idx in tokenizer.word2idx.items():\n",
    "        if word in word_embeddings:\n",
    "            embedding_matrix[idx] = word_embeddings[word]\n",
    "            found += 1\n",
    "    \n",
    "    print(f\"Found embeddings for {found}/{vocab_size-2} words ({found/(vocab_size-2)*100:.2f}%)\")\n",
    "    return embedding_matrix\n",
    "\n",
    "# Convert Pandas Series to lists first\n",
    "train_texts_list = train_texts.tolist()\n",
    "dev_texts_list = dev_texts.tolist()\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(train_labels)\n",
    "y_dev_encoded = label_encoder.transform(dev_labels)\n",
    "\n",
    "# Create sequence datasets - now passing lists instead of Series\n",
    "seq_data = create_sequence_datasets(\n",
    "    train_texts_list,  \n",
    "    dev_texts_list,    \n",
    "    y_train_encoded, \n",
    "    y_dev_encoded,\n",
    "    max_vocab=20000,\n",
    "    max_len=128\n",
    ")\n",
    "\n",
    "# Create embedding matrix\n",
    "embedding_matrix = create_embedding_matrix(\n",
    "    seq_data[\"tokenizer\"], \n",
    "    glove_embeddings, \n",
    "    embedding_dim=300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch DataLoaders for efficient batch processing:\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = TensorDataset(seq_data[\"X_train\"], seq_data[\"y_train\"])\n",
    "dev_dataset = TensorDataset(seq_data[\"X_dev\"], seq_data[\"y_dev\"])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: RNN Training Loop (B.5)\n",
    "Evaluate the RNN using dev data and compute relevant classification metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(model, train_loader, dev_loader, epochs=10, lr=0.001):\n",
    "    \"\"\"Train RNN model with early stopping based on dev set performance\"\"\"\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # For early stopping\n",
    "    best_valid_loss = float('inf')\n",
    "    best_valid_acc = 0\n",
    "    patience = 3\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(f\"Training for {epochs} epochs...\")\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "        # Calculate average loss and accuracy\n",
    "        train_loss = train_loss / train_total\n",
    "        train_acc = train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        valid_correct = 0\n",
    "        valid_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dev_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                valid_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                valid_total += labels.size(0)\n",
    "                valid_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        valid_loss = valid_loss / valid_total\n",
    "        valid_acc = valid_correct / valid_total\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc:.4f}\")\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if valid_acc > best_valid_acc:\n",
    "            best_valid_acc = valid_acc\n",
    "            best_valid_loss = valid_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, best_valid_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: RNN Evaluation (B.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rnn(model, data_loader, label_encoder, title=\"Confusion Matrix\"):\n",
    "    \"\"\"Evaluate RNN model performance\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "    precision = precision_score(all_labels, all_preds, average=\"macro\")\n",
    "    recall = recall_score(all_labels, all_preds, average=\"macro\")\n",
    "    \n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    \n",
    "    # Full classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=label_encoder.classes_,\n",
    "                yticklabels=label_encoder.classes_)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_embedding_experiments(seq_data, label_encoder, embedding_configs):\n",
    "    \"\"\"\n",
    "    Runs experiments with different embedding configurations\n",
    "    \n",
    "    Args:\n",
    "        seq_data: Dictionary containing tokenizer and dataset tensors\n",
    "        label_encoder: The label encoder for class names\n",
    "        embedding_configs: List of dictionaries with configurations\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(seq_data[\"X_train\"], seq_data[\"y_train\"]), \n",
    "        batch_size=64, \n",
    "        shuffle=True\n",
    "    )\n",
    "    dev_loader = DataLoader(\n",
    "        TensorDataset(seq_data[\"X_dev\"], seq_data[\"y_dev\"]), \n",
    "        batch_size=64\n",
    "    )\n",
    "    \n",
    "    for config in embedding_configs:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Running experiment: {config['name']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Create model with this config\n",
    "        model = RNNClassifier(\n",
    "            vocab_size=seq_data[\"tokenizer\"].get_vocab_size(),\n",
    "            embedding_dim=config['embedding_dim'],\n",
    "            hidden_dim=256,\n",
    "            output_dim=len(label_encoder.classes_),\n",
    "            embeddings_matrix=config['embedding_matrix'],\n",
    "            rnn_type=\"lstm\",  # Keep architecture consistent for comparison\n",
    "            bidirectional=True,\n",
    "            n_layers=2,\n",
    "            dropout=0.3,\n",
    "            freeze_embeddings=config['freeze_embeddings']\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        model, best_acc = train_rnn(\n",
    "            model, \n",
    "            train_loader, \n",
    "            dev_loader, \n",
    "            epochs=10,  # Reduce for faster experimentation\n",
    "            lr=0.001\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        metrics = evaluate_rnn(\n",
    "            model, \n",
    "            dev_loader, \n",
    "            label_encoder,\n",
    "            title=f\"{config['name']} Performance\"\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'name': config['name'],\n",
    "            'freeze': config['freeze_embeddings'],\n",
    "            'type': config['embedding_type'],\n",
    "            'accuracy': metrics['accuracy'],\n",
    "            'f1': metrics['f1'],\n",
    "            'precision': metrics['precision'],\n",
    "            'recall': metrics['recall']\n",
    "        })\n",
    "        \n",
    "    # Create results dataframe\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nResults Summary:\")\n",
    "    print(results_df.sort_values(by=\"f1\", ascending=False))\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Group by embedding type\n",
    "    grouped = results_df.groupby(['type', 'freeze'])\n",
    "    \n",
    "    # Set up bar positions\n",
    "    bar_width = 0.15\n",
    "    r1 = np.arange(len(results_df['type'].unique()))\n",
    "    \n",
    "    # Colors for frozen vs fine-tuned\n",
    "    colors = ['#1f77b4', '#ff7f0e']\n",
    "    \n",
    "    # Plot bars\n",
    "    i = 0\n",
    "    for (emb_type, freeze), group in grouped:\n",
    "        offset = -0.2 if not freeze else 0.2\n",
    "        plt.bar(r1[results_df['type'].unique().tolist().index(emb_type)] + offset, \n",
    "                group['f1'].values[0], \n",
    "                width=bar_width, \n",
    "                label=f\"{emb_type} ({'Frozen' if freeze else 'Fine-tuned'})\",\n",
    "                color=colors[i % 2])\n",
    "        i += 1\n",
    "    \n",
    "    plt.xlabel('Embedding Type')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('Embedding Type and Training Strategy Comparison')\n",
    "    plt.xticks(r1, results_df['type'].unique())\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Function to setup all experiments in one go\n",
    "def setup_all_embedding_experiments(train_texts, dev_texts, train_labels, dev_labels):\n",
    "    # Convert to lists if they're pandas Series\n",
    "    if hasattr(train_texts, 'tolist'):\n",
    "        train_texts = train_texts.tolist()\n",
    "    if hasattr(dev_texts, 'tolist'):\n",
    "        dev_texts = dev_texts.tolist()\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(train_labels)\n",
    "    y_dev_encoded = label_encoder.transform(dev_labels)\n",
    "    \n",
    "    # Create sequence datasets - using same tokenizer for all experiments\n",
    "    seq_data = create_sequence_datasets(\n",
    "        train_texts,\n",
    "        dev_texts,\n",
    "        y_train_encoded,\n",
    "        y_dev_encoded,\n",
    "        max_vocab=20000,\n",
    "        max_len=128\n",
    "    )\n",
    "    \n",
    "    # Load different embeddings\n",
    "    glove_embeddings = load_glove_embeddings(\"data/glove.6B.300d.txt\", embedding_dim=300)\n",
    "    word2vec_embeddings = load_word2vec_embeddings(embedding_dim=300)\n",
    "    fasttext_embeddings = load_fasttext_embeddings(\n",
    "        path=\"data/cc.en.300.vec\", embedding_dim=300\n",
    "    )\n",
    "    \n",
    "    # Create embedding matrices\n",
    "    glove_matrix = create_embedding_matrix(\n",
    "        seq_data[\"tokenizer\"], glove_embeddings, embedding_dim=300\n",
    "    )\n",
    "    \n",
    "    # Create Word2Vec and FastText matrices with proper handling if not available\n",
    "    word2vec_matrix = None\n",
    "    fasttext_matrix = None\n",
    "    \n",
    "    if word2vec_embeddings:\n",
    "        # Only create if embeddings were loaded successfully\n",
    "        word2vec_matrix = create_embedding_matrix(\n",
    "            seq_data[\"tokenizer\"], word2vec_embeddings, embedding_dim=300\n",
    "        )\n",
    "    \n",
    "    if fasttext_embeddings:\n",
    "        # Only create if embeddings were loaded successfully\n",
    "        fasttext_matrix = create_embedding_matrix(\n",
    "            seq_data[\"tokenizer\"], fasttext_embeddings, embedding_dim=300\n",
    "        )\n",
    "    \n",
    "    # Define experiment configs\n",
    "    embedding_configs = [\n",
    "        {\n",
    "            'name': 'GloVe 300d (Fine-tuned)',\n",
    "            'embedding_type': 'GloVe',\n",
    "            'embedding_dim': 300,\n",
    "            'embedding_matrix': glove_matrix,\n",
    "            'freeze_embeddings': False\n",
    "        },\n",
    "        {\n",
    "            'name': 'GloVe 300d (Frozen)',\n",
    "            'embedding_type': 'GloVe',\n",
    "            'embedding_dim': 300,\n",
    "            'embedding_matrix': glove_matrix,\n",
    "            'freeze_embeddings': True\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Add Word2Vec configs if available\n",
    "    if word2vec_matrix is not None:\n",
    "        embedding_configs.extend([\n",
    "            {\n",
    "                'name': 'Word2Vec 300d (Fine-tuned)',\n",
    "                'embedding_type': 'Word2Vec',\n",
    "                'embedding_dim': 300,\n",
    "                'embedding_matrix': word2vec_matrix,\n",
    "                'freeze_embeddings': False\n",
    "            },\n",
    "            {\n",
    "                'name': 'Word2Vec 300d (Frozen)',\n",
    "                'embedding_type': 'Word2Vec',\n",
    "                'embedding_dim': 300,\n",
    "                'embedding_matrix': word2vec_matrix,\n",
    "                'freeze_embeddings': True\n",
    "            }\n",
    "        ])\n",
    "    \n",
    "    # Add FastText configs if available\n",
    "    if fasttext_matrix is not None:\n",
    "        embedding_configs.extend([\n",
    "            {\n",
    "                'name': 'FastText 300d (Fine-tuned)',\n",
    "                'embedding_type': 'FastText',\n",
    "                'embedding_dim': 300,\n",
    "                'embedding_matrix': fasttext_matrix,\n",
    "                'freeze_embeddings': False\n",
    "            },\n",
    "            {\n",
    "                'name': 'FastText 300d (Frozen)',\n",
    "                'embedding_type': 'FastText',\n",
    "                'embedding_dim': 300,\n",
    "                'embedding_matrix': fasttext_matrix,\n",
    "                'freeze_embeddings': True\n",
    "            }\n",
    "        ])\n",
    "    \n",
    "    # Add a random embedding baseline for comparison\n",
    "    random_matrix = np.random.uniform(-0.1, 0.1, (seq_data[\"tokenizer\"].get_vocab_size(), 300))\n",
    "    embedding_configs.append({\n",
    "        'name': 'Random 300d (Fine-tuned)',\n",
    "        'embedding_type': 'Random',\n",
    "        'embedding_dim': 300,\n",
    "        'embedding_matrix': random_matrix,\n",
    "        'freeze_embeddings': False\n",
    "    })\n",
    "    \n",
    "    return seq_data, label_encoder, embedding_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all embedding experiments\n",
    "seq_data, label_encoder, embedding_configs = setup_all_embedding_experiments(\n",
    "    train_texts, \n",
    "    dev_texts, \n",
    "    train_labels, \n",
    "    dev_labels\n",
    ")\n",
    "\n",
    "# Run a subset of experiments for time/resource constraints\n",
    "# You can comment this out to run all experiments\n",
    "embedding_configs = embedding_configs[:3]  # Just run the first 3 configs for demo\n",
    "\n",
    "# Run experiments\n",
    "results_df = run_embedding_experiments(seq_data, label_encoder, embedding_configs)\n",
    "\n",
    "# Print comparative analysis\n",
    "print(\"\\nKey Findings:\")\n",
    "for emb_type in results_df['type'].unique():\n",
    "    # Compare frozen vs fine-tuned for each embedding type\n",
    "    type_results = results_df[results_df['type'] == emb_type]\n",
    "    if len(type_results) > 1:  # Only if we have both frozen and fine-tuned\n",
    "        frozen = type_results[type_results['freeze'] == True]\n",
    "        finetuned = type_results[type_results['freeze'] == False]\n",
    "        \n",
    "        if not frozen.empty and not finetuned.empty:\n",
    "            diff = finetuned['f1'].values[0] - frozen['f1'].values[0]\n",
    "            \n",
    "            print(f\"\\n{emb_type} embeddings:\")\n",
    "            print(f\"  - Fine-tuned F1: {finetuned['f1'].values[0]:.4f}\")\n",
    "            print(f\"  - Frozen F1:     {frozen['f1'].values[0]:.4f}\")\n",
    "            print(f\"  - Difference:    {diff:.4f} ({'better with fine-tuning' if diff > 0 else 'better frozen'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visual comparison of all models\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Set up multi-bar chart\n",
    "def grouped_bar_chart(results_df, metrics=['accuracy', 'f1', 'precision', 'recall']):\n",
    "    # Get unique model names\n",
    "    models = results_df['name'].unique()\n",
    "    \n",
    "    # Number of metrics\n",
    "    n_metrics = len(metrics)\n",
    "    \n",
    "    # Set width of bars\n",
    "    bar_width = 0.8 / n_metrics\n",
    "    \n",
    "    # Set positions for bars on x-axis\n",
    "    positions = np.arange(len(models))\n",
    "    \n",
    "    # Plot bars\n",
    "    for i, metric in enumerate(metrics):\n",
    "        offset = bar_width * i - (bar_width * (n_metrics-1))/2\n",
    "        plt.bar(positions + offset, \n",
    "                results_df[metric], \n",
    "                width=bar_width, \n",
    "                label=metric.capitalize())\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Model Configuration')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Performance Comparison of Different Embedding Strategies')\n",
    "    plt.xticks(positions, models, rotation=45, ha='right')\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add value labels on the bars\n",
    "    for i, metric in enumerate(metrics):\n",
    "        offset = bar_width * i - (bar_width * (n_metrics-1))/2\n",
    "        for j, value in enumerate(results_df[metric]):\n",
    "            plt.text(j + offset, value + 0.01, f'{value:.3f}', \n",
    "                     ha='center', va='bottom', fontsize=8, rotation=90)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "grouped_bar_chart(results_df, metrics=['accuracy', 'f1'])\n",
    "\n",
    "# Create a summary heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "pivot_df = results_df.pivot_table(\n",
    "    index='type',\n",
    "    columns='freeze', \n",
    "    values='f1',\n",
    "    aggfunc='first'\n",
    ")\n",
    "sns.heatmap(pivot_df, annot=True, cmap='viridis', fmt='.4f')\n",
    "plt.title('F1 Score by Embedding Type and Training Strategy')\n",
    "plt.xlabel('Freeze Embeddings')\n",
    "plt.ylabel('Embedding Type')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
