{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAT550 - Group Project\n",
    "### Bag of Words Document Classification using Feedforward Neural Network and Recurrent Neural Network\n",
    "Authors: Andreas etternavn, Haakon Vollheim Webb, H√•kon Nodeland, Stian etternavn\n",
    "\n",
    "\n",
    "\n",
    "The code has the following structure:\n",
    "\n",
    "1. Defining imports\n",
    "2. Reading data\n",
    "3. Cleaning data\n",
    "4. Preprocessing data\n",
    "5. Defining the model\n",
    "6. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Project 2: Bag-of-Words Document Classification with Feedforward and Recurrent Neural Networks\n",
    "\n",
    "## üéØ Objective\n",
    "Train and evaluate Feedforward Neural Networks (FFNN) and Recurrent Neural Networks (RNN) on a multiclass document classification task using various bag-of-words (BoW) feature extraction methods.\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ Dataset Information\n",
    "\n",
    "- **Source**: A subset of the ArXiv-10 dataset ([Dataset Link](https://paperswithcode.com/dataset/arxiv-10))\n",
    "- **Structure**: \n",
    "  - `abstract` (input feature)\n",
    "  - `field` (target label)\n",
    "- **Task**: Predict the field of research (10-class classification) from the article abstract.\n",
    "\n",
    "---\n",
    "\n",
    "# üó∫Ô∏è Workflow Overview\n",
    "\n",
    "Use the links below to jump directly to the relevant code sections.\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Part 0: Common Preprocessing and Setup (Shared)\n",
    "- [üß© Code: Imports and Config (0.1)](#Ô∏ècode-imports-and-config-01)\n",
    "- [üß© Code: Load and Inspect Dataset (0.2)](#code-load-and-inspect-dataset-02)\n",
    "- [üß© Code: Train-Dev Split (0.3)](#code-train-dev-split-03)\n",
    "- [üß© Code: Text Preprocessing (0.4)](#code-text-preprocessing-04)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ú® Part A: Feedforward Neural Network with Bag-of-Words (H√•kon og Haakon)\n",
    "- [üß© Code: Feature Extraction ‚Äì Bag of Words (A.1)](#code-feature-extraction--bag-of-words-a1)\n",
    "- [üß© Code: FFNN Model Definition (A.2)](#code-ffnn-model-definition-a2)\n",
    "- [üß© Code: FFNN Training Loop (A.3)](#code-ffnn-training-loop-a3)\n",
    "- [üß© Code: FFNN Evaluation on Dev Set (A.4)](#code-ffnn-evaluation-on-dev-set-a4)\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Part B: Recurrent Neural Network with Pre-trained Embeddings (Andreas og Stian)\n",
    "- [üß© Code: Load Pre-trained Word Embeddings (B.1)](#code-load-pre-trained-word-embeddings-b1)\n",
    "- [üß© Code: Text-to-Sequence Pipeline (B.2)](#code-text-to-sequence-pipeline-b2)\n",
    "- [üß© Code: RNN Model Definition (B.3)](#code-rnn-model-definition-b3)\n",
    "- [üß© Code: RNN Training Loop (B.4)](#code-rnn-training-loop-b4)\n",
    "- [üß© Code: RNN Evaluation on Dev Set (B.5)](#code-rnn-evaluation-on-dev-set-b5)\n",
    "\n",
    "---\n",
    "\n",
    "# üßπ Part 0: Common Preprocessing and Setup (Both Must Use)\n",
    "\n",
    "## üîß 0.1: Imports and Config\n",
    "\n",
    "- Load libraries (NumPy, pandas, PyTorch, scikit-learn, etc.)\n",
    "- Set random seed for reproducibility\n",
    "\n",
    "## üìÑ 0.2: Load and Inspect Dataset\n",
    "\n",
    "- Load the dataset from `.csv.gz` file\n",
    "- Print sample rows, label distribution\n",
    "\n",
    "## üß™ 0.3: Train-Dev Split\n",
    "\n",
    "- Split the data into **training** and **development** sets\n",
    "- Use **stratified splitting** to preserve label distribution\n",
    "\n",
    "### üìä Purpose of Train/Dev Split\n",
    "\n",
    "We split the dataset into **training** and **development (dev)** sets to evaluate how well the model generalizes to unseen data.\n",
    "\n",
    "- **Train Set**: Used to fit the model and update weights.\n",
    "- **Dev Set**: Used to tune hyperparameters and compare model configurations without overfitting.\n",
    "\n",
    "> ‚úÖ This separation ensures we don‚Äôt evaluate on data the model has already seen, giving us a more honest estimate of its real-world performance.\n",
    "\n",
    "\n",
    "## ‚úèÔ∏è 0.4: Text Preprocessing\n",
    "\n",
    "- Clean text (lowercasing, punctuation removal, optional stopwords)\n",
    "- Tokenization (if needed for embedding-based methods)\n",
    "\n",
    "---\n",
    "\n",
    "# ‚ú® Part A: Feedforward Neural Network with Bag-of-Words\n",
    "\n",
    "### üë§ Assigned to: **H√•kon og Haakon**\n",
    "\n",
    "## üìä A.1: Feature Extraction ‚Äì Bag of Words\n",
    "\n",
    "- Use two different vectorization techniques:\n",
    "- `CountVectorizer`\n",
    "- `TfidfVectorizer`\n",
    "- Optionally adjust:\n",
    "- `ngram_range`\n",
    "- `min_df`, `max_df`\n",
    "- `max_features`\n",
    "\n",
    "## üß† A.2: Model ‚Äì Feedforward Neural Network (MLP)\n",
    "\n",
    "- Design multiple MLP architectures with:\n",
    "- 1 hidden layer\n",
    "- 2+ hidden layers\n",
    "- Use BoW feature vectors as input\n",
    "\n",
    "## üèãÔ∏è A.3: Training\n",
    "\n",
    "- Use `CrossEntropyLoss` as the loss function\n",
    "- Optimizer: Adam or SGD\n",
    "- Track loss and accuracy per epoch\n",
    "\n",
    "## üìà A.4: Evaluation on Dev Set\n",
    "\n",
    "- Evaluate using:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- Macro-F1 Score\n",
    "- Plot confusion matrix\n",
    "- Compare models using both BoW types\n",
    "\n",
    "## üìë A.5: Summary\n",
    "\n",
    "- Compare TF-IDF vs CountVectorizer\n",
    "- Discuss impact of model depth\n",
    "- Reflect on overfitting/underfitting, training time, etc.\n",
    "\n",
    "---\n",
    "\n",
    "# üîÅ Part B: Recurrent Neural Network with Pre-trained Embeddings\n",
    "\n",
    "### üë§ Assigned to: **Andreas og Stian**\n",
    "\n",
    "## üî° B.1: Load Pre-trained Word Embeddings\n",
    "\n",
    "- Choose at least one:\n",
    "- Word2Vec\n",
    "- GloVe\n",
    "- FastText\n",
    "- Use pre-trained embeddings or train your own on external corpus (optional)\n",
    "\n",
    "## üß© B.2: Text-to-Sequence Pipeline\n",
    "\n",
    "- Tokenize each abstract into word indices\n",
    "- Convert each word to embedding\n",
    "- Pad/truncate sequences to same length\n",
    "\n",
    "## üß† B.3: RNN-Based Classifier\n",
    "\n",
    "- Use PyTorch to build models with:\n",
    "- Simple RNN\n",
    "- LSTM\n",
    "- GRU\n",
    "- Vary architectures:\n",
    "- Hidden state sizes\n",
    "- Layers\n",
    "- Bidirectional RNNs\n",
    "\n",
    "## üèãÔ∏è B.4: Training\n",
    "\n",
    "- Use `CrossEntropyLoss`\n",
    "- Optimizer: Adam\n",
    "- Monitor training loss and accuracy\n",
    "\n",
    "## üìà B.5: Evaluation on Dev Set\n",
    "\n",
    "- Metrics:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- Macro-F1 Score\n",
    "- Try different sequence pooling methods:\n",
    "- Last hidden state\n",
    "- Max/mean pooling\n",
    "- BiRNN concatenation\n",
    "\n",
    "## üìë B.6: Summary\n",
    "\n",
    "- Compare performance across embedding models and RNN types\n",
    "- Discuss tradeoffs (training time, performance, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Code: Imports and Config (0.1)\n",
    "Set up the required Python libraries and configure global settings (e.g., seed, device)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# ‚úÖ Config\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())  # True if your system is GPU-ready\n",
    "print(torch.cuda.get_device_name(0))  # Name of your GPU (if available)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Code: Load and Inspect Dataset (0.2)\n",
    "Load the compressed dataset and inspect its structure, size, and label distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/arxiv100.csv\")\n",
    "\n",
    "# Basic inspection\n",
    "print(df.head())\n",
    "print(df[\"label\"].value_counts())\n",
    "print(f\"Dataset size: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Code: Train-Dev Split (0.3)\n",
    "Split the dataset into training and development sets using stratified sampling to preserve class balance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Stratified split on label\n",
    "train_texts, dev_texts, train_labels, dev_labels = train_test_split(\n",
    "    df[\"abstract\"], df[\"label\"], \n",
    "    test_size=0.2, \n",
    "    stratify=df[\"label\"], \n",
    "    random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Code: Text Preprocessing (0.4)\n",
    "Clean the abstract text (lowercasing, removing punctuation, etc.) and prepare it for feature extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower() # Converts to lower\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text) # Removes punctuation and special characters\n",
    "    text = re.sub(r\"\\d+\", \"\", text) # Removes numbers\n",
    "    return text\n",
    "\n",
    "train_texts = train_texts.apply(clean_text)\n",
    "dev_texts = dev_texts.apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Code: Feature Extraction ‚Äì Bag of Words (A.1)\n",
    "Use `CountVectorizer` and `TfidfVectorizer` to convert abstracts into numerical feature vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# === Label encoding (shared across both vectorizers) ===\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(train_labels)\n",
    "y_dev_encoded = label_encoder.transform(dev_labels)\n",
    "\n",
    "# === Bag-of-Words Vectorization ===\n",
    "vectorizers = {\n",
    "    \"count\": CountVectorizer(max_features=10000, ngram_range=(1, 2)),\n",
    "    \"tfidf\": TfidfVectorizer(max_features=10000, ngram_range=(1, 2)),\n",
    "}\n",
    "\n",
    "# Store the feature vectors in a dict for easy comparison\n",
    "bow_features = {}\n",
    "\n",
    "for name, vectorizer in vectorizers.items():\n",
    "    print(f\"Fitting and transforming with {name} vectorizer...\")\n",
    "    X_train_vec = vectorizer.fit_transform(train_texts)\n",
    "    X_dev_vec = vectorizer.transform(dev_texts)\n",
    "\n",
    "    # Convert to dense arrays for PyTorch usage\n",
    "    X_train_array = X_train_vec.toarray()\n",
    "    X_dev_array = X_dev_vec.toarray()\n",
    "\n",
    "    bow_features[name] = {\n",
    "        \"X_train\": X_train_array,\n",
    "        \"X_dev\": X_dev_array,\n",
    "        \"y_train\": y_train_encoded,\n",
    "        \"y_dev\": y_dev_encoded,\n",
    "        \"vectorizer\": vectorizer,\n",
    "    }\n",
    "\n",
    "print(\"Done vectorizing with CountVectorizer and TF-IDF.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Code: FFNN Model Definition (A.2)\n",
    "Define one or more fully connected feedforward neural networks (MLPs) using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, \n",
    "                 use_batchnorm=False, use_dropout=True, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            if use_dropout:\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_dim = h_dim\n",
    "\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Code: FFNN Training Loop (A.3)\n",
    "Implement the training loop for the MLP models, including loss calculation and optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def train_ffnn(\n",
    "    model, \n",
    "    X_train, y_train, \n",
    "    X_dev, y_dev, \n",
    "    epochs=10, \n",
    "    batch_size=64, \n",
    "    lr=1e-3,\n",
    "    use_weight_init=True,\n",
    "    verbose=True\n",
    "):\n",
    "    # Move model to device\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    # Optional: custom weight initialization\n",
    "    if use_weight_init:\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "        model.apply(init_weights)\n",
    "\n",
    "    # Prepare data loaders\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.long)\n",
    "    )\n",
    "    dev_dataset = TensorDataset(\n",
    "        torch.tensor(X_dev, dtype=torch.float32),\n",
    "        torch.tensor(y_dev, dtype=torch.long)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Set loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * y_batch.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "        train_loss = total_loss / total\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # Evaluate on dev set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            dev_correct = 0\n",
    "            dev_total = 0\n",
    "            for X_dev_batch, y_dev_batch in dev_loader:\n",
    "                X_dev_batch, y_dev_batch = X_dev_batch.to(DEVICE), y_dev_batch.to(DEVICE)\n",
    "                dev_outputs = model(X_dev_batch)\n",
    "                dev_preds = dev_outputs.argmax(dim=1)\n",
    "                dev_correct += (dev_preds == y_dev_batch).sum().item()\n",
    "                dev_total += y_dev_batch.size(0)\n",
    "\n",
    "        dev_acc = dev_correct / dev_total\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Dev Acc: {dev_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FFNNClassifier(\n",
    "    input_dim=bow_features[\"count\"][\"X_train\"].shape[1],\n",
    "    hidden_dims=[512, 256],\n",
    "    output_dim=len(label_encoder.classes_),\n",
    "    use_batchnorm=True,\n",
    "    use_dropout=True\n",
    ")\n",
    "\n",
    "train_ffnn(\n",
    "    model=model,\n",
    "    X_train=bow_features[\"count\"][\"X_train\"],\n",
    "    y_train=bow_features[\"count\"][\"y_train\"],\n",
    "    X_dev=bow_features[\"count\"][\"X_dev\"],\n",
    "    y_dev=bow_features[\"count\"][\"y_dev\"],\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    lr=1e-3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Code: FFNN Evaluation on Dev Set (A.4)\n",
    "Evaluate the trained MLPs on the dev set using accuracy, precision, recall, and macro-F1 score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ffnn(model, X, y_true, label_encoder, title=\"Confusion Matrix\", verbose=True):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(DEVICE)\n",
    "        preds = model(X_tensor).argmax(dim=1).cpu().numpy()\n",
    "\n",
    "    # Metrics\n",
    "    y_true = np.array(y_true)\n",
    "    acc = accuracy_score(y_true, preds)\n",
    "    f1 = f1_score(y_true, preds, average=\"macro\")\n",
    "    precision = precision_score(y_true, preds, average=\"macro\")\n",
    "    recall = recall_score(y_true, preds, average=\"macro\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Classification Report:\\n\")\n",
    "        print(classification_report(y_true, preds, target_names=label_encoder.classes_))\n",
    "    \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_true, preds)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=label_encoder.classes_,\n",
    "                    yticklabels=label_encoder.classes_)\n",
    "        plt.xlabel(\"Predicted Label\")\n",
    "        plt.ylabel(\"True Label\")\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_ffnn(\n",
    "    model=model,\n",
    "    X=bow_features[\"tfidf\"][\"X_dev\"],\n",
    "    y_true=bow_features[\"tfidf\"][\"y_dev\"],\n",
    "    label_encoder=label_encoder,\n",
    "    title=\"TF-IDF (1-layer FFNN) Confusion Matrix\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Code: Load Pre-trained Word Embeddings (B.1)\n",
    "Load pre-trained embeddings (e.g., GloVe, Word2Vec) and map vocabulary to embedding vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(path=\"glove.6B.100d.txt\"):\n",
    "    embeddings = {}\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vec = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vec\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Code: Text-to-Sequence Pipeline (B.2)\n",
    "Convert preprocessed abstracts into padded sequences of word indices aligned with the embedding matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=20000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_texts)\n",
    "X_dev_seq = tokenizer.texts_to_sequences(dev_texts)\n",
    "\n",
    "max_len = 200  # or compute based on data\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
    "X_dev_pad = pad_sequences(X_dev_seq, maxlen=max_len, padding='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Code: RNN Model Definition (B.3)\n",
    "Define the RNN model architecture using PyTorch (Simple RNN, LSTM, or GRU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, rnn_type=\"lstm\"):\n",
    "        super().__init__()\n",
    "        vocab_size, emb_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=False)\n",
    "\n",
    "        if rnn_type == \"gru\":\n",
    "            self.rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden, _) = self.rnn(x)\n",
    "        out = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)  # BiRNN\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Code: RNN Training Loop (B.4)\n",
    "Train the RNN on sequence data, tracking loss and accuracy across epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(model, X_train, y_train, X_dev, y_dev, epochs=10, lr=1e-3):\n",
    "    model.to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_data = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.long),\n",
    "        torch.tensor(y_train, dtype=torch.long)\n",
    "    )\n",
    "    loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Code: RNN Evaluation on Dev Set (B.5)\n",
    "Evaluate the RNN using dev data and compute relevant classification metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rnn(model, X, y, label_encoder):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(torch.tensor(X, dtype=torch.long).to(DEVICE)).argmax(dim=1).cpu().numpy()\n",
    "    print(classification_report(y, preds, target_names=label_encoder.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
