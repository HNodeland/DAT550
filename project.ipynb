{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAT550 - Group Project\n",
    "### Bag of Words Document Classification using Feedforward Neural Network and Recurrent Neural Network\n",
    "Authors: Andreas etternavn, Haakon Vollheim Webb, HÃ¥kon Nodeland, Stian etternavn\n",
    "\n",
    "\n",
    "\n",
    "The code has the following structure:\n",
    "\n",
    "1. Defining imports\n",
    "2. Reading data\n",
    "3. Cleaning data\n",
    "4. Preprocessing data\n",
    "5. Defining the model\n",
    "6. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  Project 2: Bag-of-Words Document Classification with Feedforward and Recurrent Neural Networks\n",
    "\n",
    "## ðŸŽ¯ Objective\n",
    "Train and evaluate Feedforward Neural Networks (FFNN) and Recurrent Neural Networks (RNN) on a multiclass document classification task using various bag-of-words (BoW) feature extraction methods.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Dataset Information\n",
    "\n",
    "- **Source**: A subset of the ArXiv-10 dataset ([Dataset Link](https://paperswithcode.com/dataset/arxiv-10))\n",
    "- **Structure**: \n",
    "  - `abstract` (input feature)\n",
    "  - `field` (target label)\n",
    "- **Task**: Predict the field of research (10-class classification) from the article abstract.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ—ºï¸ Workflow Overview\n",
    "\n",
    "Use the links below to jump directly to the relevant code sections.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¹ Part 0: Common Preprocessing and Setup (Shared)\n",
    "- [ðŸ§© Code: Imports and Config (0.1)](#ï¸code-imports-and-config-01)\n",
    "- [ðŸ§© Code: Load and Inspect Dataset (0.2)](#code-load-and-inspect-dataset-02)\n",
    "- [ðŸ§© Code: Train-Dev Split (0.3)](#code-train-dev-split-03)\n",
    "- [ðŸ§© Code: Text Preprocessing (0.4)](#code-text-preprocessing-04)\n",
    "\n",
    "---\n",
    "\n",
    "## âœ¨ Part A: Feedforward Neural Network with Bag-of-Words (HÃ¥kon og Haakon)\n",
    "- [ðŸ§© Code: Feature Extraction â€“ Bag of Words (A.1)](#code-feature-extraction--bag-of-words-a1)\n",
    "- [ðŸ§© Code: FFNN Model Definition (A.2)](#code-ffnn-model-definition-a2)\n",
    "- [ðŸ§© Code: FFNN Training Loop (A.3)](#code-ffnn-training-loop-a3)\n",
    "- [ðŸ§© Code: FFNN Evaluation on Dev Set (A.4)](#code-ffnn-evaluation-on-dev-set-a4)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Part B: Recurrent Neural Network with Pre-trained Embeddings (Andreas og Stian)\n",
    "- [ðŸ§© Code: Load Pre-trained Word Embeddings (B.1)](#code-load-pre-trained-word-embeddings-b1)\n",
    "- [ðŸ§© Code: Text-to-Sequence Pipeline (B.2)](#code-text-to-sequence-pipeline-b2)\n",
    "- [ðŸ§© Code: RNN Model Definition (B.3)](#code-rnn-model-definition-b3)\n",
    "- [ðŸ§© Code: RNN Training Loop (B.4)](#code-rnn-training-loop-b4)\n",
    "- [ðŸ§© Code: RNN Evaluation on Dev Set (B.5)](#code-rnn-evaluation-on-dev-set-b5)\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ§¹ Part 0: Common Preprocessing and Setup (Both Must Use)\n",
    "\n",
    "## ðŸ”§ 0.1: Imports and Config\n",
    "\n",
    "- Load libraries (NumPy, pandas, PyTorch, scikit-learn, etc.)\n",
    "- Set random seed for reproducibility\n",
    "\n",
    "## ðŸ“„ 0.2: Load and Inspect Dataset\n",
    "\n",
    "- Load the dataset from `.csv.gz` file\n",
    "- Print sample rows, label distribution\n",
    "\n",
    "## ðŸ§ª 0.3: Train-Dev Split\n",
    "\n",
    "- Split the data into **training** and **development** sets\n",
    "- Use **stratified splitting** to preserve label distribution\n",
    "\n",
    "### ðŸ“Š Purpose of Train/Dev Split\n",
    "\n",
    "We split the dataset into **training** and **development (dev)** sets to evaluate how well the model generalizes to unseen data.\n",
    "\n",
    "- **Train Set**: Used to fit the model and update weights.\n",
    "- **Dev Set**: Used to tune hyperparameters and compare model configurations without overfitting.\n",
    "\n",
    "> âœ… This separation ensures we donâ€™t evaluate on data the model has already seen, giving us a more honest estimate of its real-world performance.\n",
    "\n",
    "\n",
    "## âœï¸ 0.4: Text Preprocessing\n",
    "\n",
    "- Clean text (lowercasing, punctuation removal, optional stopwords)\n",
    "- Tokenization (if needed for embedding-based methods)\n",
    "\n",
    "---\n",
    "\n",
    "# âœ¨ Part A: Feedforward Neural Network with Bag-of-Words\n",
    "\n",
    "### ðŸ‘¤ Assigned to: **HÃ¥kon og Haakon**\n",
    "\n",
    "## ðŸ“Š A.1: Feature Extraction â€“ Bag of Words\n",
    "\n",
    "- Use two different vectorization techniques:\n",
    "- `CountVectorizer`\n",
    "- `TfidfVectorizer`\n",
    "- Optionally adjust:\n",
    "- `ngram_range`\n",
    "- `min_df`, `max_df`\n",
    "- `max_features`\n",
    "\n",
    "## ðŸ§  A.2: Model â€“ Feedforward Neural Network (MLP)\n",
    "\n",
    "- Design multiple MLP architectures with:\n",
    "- 1 hidden layer\n",
    "- 2+ hidden layers\n",
    "- Use BoW feature vectors as input\n",
    "\n",
    "## ðŸ‹ï¸ A.3: Training\n",
    "\n",
    "- Use `CrossEntropyLoss` as the loss function\n",
    "- Optimizer: Adam or SGD\n",
    "- Track loss and accuracy per epoch\n",
    "\n",
    "## ðŸ“ˆ A.4: Evaluation on Dev Set\n",
    "\n",
    "- Evaluate using:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- Macro-F1 Score\n",
    "- Plot confusion matrix\n",
    "- Compare models using both BoW types\n",
    "\n",
    "## ðŸ“‘ A.5: Summary\n",
    "\n",
    "- Compare TF-IDF vs CountVectorizer\n",
    "- Discuss impact of model depth\n",
    "- Reflect on overfitting/underfitting, training time, etc.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ” Part B: Recurrent Neural Network with Pre-trained Embeddings\n",
    "\n",
    "### ðŸ‘¤ Assigned to: **Andreas og Stian**\n",
    "\n",
    "## ðŸ”¡ B.1: Load Pre-trained Word Embeddings\n",
    "\n",
    "- Choose at least one:\n",
    "- Word2Vec\n",
    "- GloVe\n",
    "- FastText\n",
    "- Use pre-trained embeddings or train your own on external corpus (optional)\n",
    "\n",
    "## ðŸ§© B.2: Text-to-Sequence Pipeline\n",
    "\n",
    "- Tokenize each abstract into word indices\n",
    "- Convert each word to embedding\n",
    "- Pad/truncate sequences to same length\n",
    "\n",
    "## ðŸ§  B.3: RNN-Based Classifier\n",
    "\n",
    "- Use PyTorch to build models with:\n",
    "- Simple RNN\n",
    "- LSTM\n",
    "- GRU\n",
    "- Vary architectures:\n",
    "- Hidden state sizes\n",
    "- Layers\n",
    "- Bidirectional RNNs\n",
    "\n",
    "## ðŸ‹ï¸ B.4: Training\n",
    "\n",
    "- Use `CrossEntropyLoss`\n",
    "- Optimizer: Adam\n",
    "- Monitor training loss and accuracy\n",
    "\n",
    "## ðŸ“ˆ B.5: Evaluation on Dev Set\n",
    "\n",
    "- Metrics:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- Macro-F1 Score\n",
    "- Try different sequence pooling methods:\n",
    "- Last hidden state\n",
    "- Max/mean pooling\n",
    "- BiRNN concatenation\n",
    "\n",
    "## ðŸ“‘ B.6: Summary\n",
    "\n",
    "- Compare performance across embedding models and RNN types\n",
    "- Discuss tradeoffs (training time, performance, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: Imports and Config (0.1)\n",
    "Set up the required Python libraries and configure global settings (e.g., seed, device)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "# âœ… Config\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())  # True if your system is GPU-ready\n",
    "print(torch.cuda.get_device_name(0))  # Name of your GPU (if available)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: Load and Inspect Dataset (0.2)\n",
    "Load the compressed dataset and inspect its structure, size, and label distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/arxiv100.csv\")\n",
    "\n",
    "# Basic inspection\n",
    "print(df.head())\n",
    "print(df[\"label\"].value_counts())\n",
    "print(f\"Dataset size: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: Train-Dev Split (0.3)\n",
    "Split the dataset into training and development sets using stratified sampling to preserve class balance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Stratified split on label\n",
    "train_texts, dev_texts, train_labels, dev_labels = train_test_split(\n",
    "    df[\"abstract\"], df[\"label\"], \n",
    "    test_size=0.2, \n",
    "    stratify=df[\"label\"], \n",
    "    random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: Text Preprocessing (0.4)\n",
    "Clean the abstract text (lowercasing, removing punctuation, etc.) and prepare it for feature extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower() # Converts to lower\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text) # Removes punctuation and special characters\n",
    "    text = re.sub(r\"\\d+\", \"\", text) # Removes numbers\n",
    "    return text\n",
    "\n",
    "train_texts = train_texts.apply(clean_text)\n",
    "dev_texts = dev_texts.apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: Feature Extraction â€“ Bag of Words (A.1)\n",
    "Use `CountVectorizer` and `TfidfVectorizer` to convert abstracts into numerical feature vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# === Label encoding (shared across both vectorizers) ===\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(train_labels)\n",
    "y_dev_encoded = label_encoder.transform(dev_labels)\n",
    "\n",
    "# === Bag-of-Words Vectorization ===\n",
    "vectorizers = {\n",
    "    \"count\": CountVectorizer(max_features=10000, ngram_range=(1, 2)),\n",
    "    \"tfidf\": TfidfVectorizer(max_features=10000, ngram_range=(1, 2)),\n",
    "}\n",
    "\n",
    "# Store the feature vectors in a dict for easy comparison\n",
    "bow_features = {}\n",
    "\n",
    "for name, vectorizer in vectorizers.items():\n",
    "    print(f\"Fitting and transforming with {name} vectorizer...\")\n",
    "    X_train_vec = vectorizer.fit_transform(train_texts)\n",
    "    X_dev_vec = vectorizer.transform(dev_texts)\n",
    "\n",
    "    # Convert to dense arrays for PyTorch usage\n",
    "    X_train_array = X_train_vec.toarray()\n",
    "    X_dev_array = X_dev_vec.toarray()\n",
    "\n",
    "    bow_features[name] = {\n",
    "        \"X_train\": X_train_array,\n",
    "        \"X_dev\": X_dev_array,\n",
    "        \"y_train\": y_train_encoded,\n",
    "        \"y_dev\": y_dev_encoded,\n",
    "        \"vectorizer\": vectorizer,\n",
    "    }\n",
    "\n",
    "print(\"Done vectorizing with CountVectorizer and TF-IDF.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: FFNN Model Definition (A.2)\n",
    "Define one or more fully connected feedforward neural networks (MLPs) using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, \n",
    "                 use_batchnorm=False, use_dropout=True, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            if use_dropout:\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_dim = h_dim\n",
    "\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: FFNN Training Loop (A.3)\n",
    "Implement the training loop for the MLP models, including loss calculation and optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def train_ffnn(\n",
    "    model, \n",
    "    X_train, y_train, \n",
    "    X_dev, y_dev, \n",
    "    epochs=10, \n",
    "    batch_size=64, \n",
    "    lr=1e-3,\n",
    "    use_weight_init=True,\n",
    "    verbose=True,\n",
    "    save_path=None  # NEW: file path to save model\n",
    "):\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    if use_weight_init:\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "        model.apply(init_weights)\n",
    "\n",
    "    # DataLoader setup\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                                  torch.tensor(y_train, dtype=torch.long))\n",
    "    dev_dataset = TensorDataset(torch.tensor(X_dev, dtype=torch.float32),\n",
    "                                torch.tensor(y_dev, dtype=torch.long))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=batch_size)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_dev_acc = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * y_batch.size(0)\n",
    "            correct += (outputs.argmax(1) == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # Dev accuracy\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            dev_correct, dev_total = 0, 0\n",
    "            for X_dev_batch, y_dev_batch in dev_loader:\n",
    "                X_dev_batch, y_dev_batch = X_dev_batch.to(DEVICE), y_dev_batch.to(DEVICE)\n",
    "                dev_preds = model(X_dev_batch).argmax(1)\n",
    "                dev_correct += (dev_preds == y_dev_batch).sum().item()\n",
    "                dev_total += y_dev_batch.size(0)\n",
    "\n",
    "        dev_acc = dev_correct / dev_total\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Train Acc: {train_acc:.4f} | Dev Acc: {dev_acc:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if dev_acc > best_dev_acc:\n",
    "            best_dev_acc = dev_acc\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    # Save final or best model\n",
    "    if save_path is not None:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        torch.save(best_model_state, save_path)\n",
    "        print(f\"Best model saved to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: FFNN Evaluation on Dev Set (A.4)\n",
    "Evaluate the trained MLPs on the dev set using accuracy, precision, recall, and macro-F1 score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ffnn(model, X, y_true, label_encoder, title=\"Confusion Matrix\", verbose=True):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(DEVICE)\n",
    "        preds = model(X_tensor).argmax(dim=1).cpu().numpy()\n",
    "\n",
    "    # Metrics\n",
    "    y_true = np.array(y_true)\n",
    "    acc = accuracy_score(y_true, preds)\n",
    "    f1 = f1_score(y_true, preds, average=\"macro\")\n",
    "    precision = precision_score(y_true, preds, average=\"macro\")\n",
    "    recall = recall_score(y_true, preds, average=\"macro\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Classification Report:\\n\")\n",
    "        print(classification_report(y_true, preds, target_names=label_encoder.classes_))\n",
    "    \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_true, preds)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=label_encoder.classes_,\n",
    "                    yticklabels=label_encoder.classes_)\n",
    "        plt.xlabel(\"Predicted Label\")\n",
    "        plt.ylabel(\"True Label\")\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_configs = {\n",
    "    \"FFNN-1L-256\": [256],\n",
    "    \"FFNN-2L-512-256\": [512, 256]\n",
    "}\n",
    "\n",
    "vectorizers_to_test = [\"count\", \"tfidf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§ª BoW x Hidden Layer Grid Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns=[\"Vectorizer\", \"Model\", \"Accuracy\", \"F1\", \"Precision\", \"Recall\"])\n",
    "\n",
    "for vectorizer_name in vectorizers_to_test:\n",
    "    # Grab preprocessed data\n",
    "    X_train = bow_features[vectorizer_name][\"X_train\"]\n",
    "    y_train = bow_features[vectorizer_name][\"y_train\"]\n",
    "    X_dev = bow_features[vectorizer_name][\"X_dev\"]\n",
    "    y_dev = bow_features[vectorizer_name][\"y_dev\"]\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = len(label_encoder.classes_)\n",
    "\n",
    "    for model_name, hidden_dims in hidden_layer_configs.items():\n",
    "        print(f\"\\n Training {model_name} using {vectorizer_name.upper()} vectorizer...\")\n",
    "\n",
    "        # Initialize model\n",
    "        model = FFNNClassifier(\n",
    "            input_dim=input_dim,\n",
    "            hidden_dims=hidden_dims,\n",
    "            output_dim=output_dim,\n",
    "            use_batchnorm=True,\n",
    "            use_dropout=True\n",
    "        )\n",
    "\n",
    "        # Train model\n",
    "        save_path = f\"models/ffnn_{vectorizer_name}_{model_name}.pt\"\n",
    "        train_ffnn(\n",
    "            model=model,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_dev=X_dev,\n",
    "            y_dev=y_dev,\n",
    "            epochs=10,\n",
    "            save_path=save_path,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "\n",
    "        # Evaluate and store results\n",
    "        metrics = evaluate_ffnn(\n",
    "            model=model,\n",
    "            X=X_dev,\n",
    "            y_true=y_dev,\n",
    "            label_encoder=label_encoder,\n",
    "            title=f\"{vectorizer_name.upper()} - {model_name}\",\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        results_df.loc[len(results_df)] = [\n",
    "            vectorizer_name.upper(), model_name,\n",
    "            metrics[\"accuracy\"], metrics[\"f1\"],\n",
    "            metrics[\"precision\"], metrics[\"recall\"]\n",
    "        ]\n",
    "\n",
    "print(\"All models trained and evaluated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.sort_values(by=\"F1\", ascending=False)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"models/ffnn_tfidf_FFNN-2L-512-256.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: Load Pre-trained Word Embeddings (B.1)\n",
    "Load pre-trained embeddings (e.g., GloVe, Word2Vec) and map vocabulary to embedding vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from gensim.models import KeyedVectors, FastText\n",
    "\n",
    "def load_pretrained_embeddings(embedding_type=\"glove\", embedding_dim=100, limit=None):\n",
    "    \"\"\"\n",
    "    Load pre-trained word embeddings of specified type.\n",
    "    \n",
    "    Args:\n",
    "        embedding_type: One of 'glove', 'word2vec', or 'fasttext'\n",
    "        embedding_dim: Dimensionality of embeddings to load\n",
    "        limit: Optional limit on number of words to load (for testing)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping words to embedding vectors\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    \n",
    "    if embedding_type == \"glove\":\n",
    "        # Path may need adjustment based on your environment\n",
    "        path = f\"data/glove.6B.{embedding_dim}d.txt\"\n",
    "        \n",
    "        print(f\"Loading GloVe embeddings from {path}...\")\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if limit and i >= limit:\n",
    "                    break\n",
    "                values = line.strip().split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings[word] = vector\n",
    "                \n",
    "    elif embedding_type == \"word2vec\":\n",
    "        # Google's pre-trained Word2Vec model\n",
    "        path = \"data/GoogleNews-vectors-negative300.bin\"\n",
    "        \n",
    "        print(f\"Loading Word2Vec embeddings from {path}...\")\n",
    "        word2vec_model = KeyedVectors.load_word2vec_format(path, binary=True, limit=limit)\n",
    "        for word in word2vec_model.key_to_index:\n",
    "            embeddings[word] = word2vec_model[word]\n",
    "            \n",
    "    elif embedding_type == \"fasttext\":\n",
    "        # Facebook's FastText embeddings\n",
    "        path = f\"data/cc.en.{embedding_dim}.bin\"  \n",
    "        \n",
    "        print(f\"Loading FastText embeddings from {path}...\")\n",
    "        fasttext_model = FastText.load_fasttext_format(path, limit=limit)\n",
    "        for word in fasttext_model.wv.key_to_index:\n",
    "            embeddings[word] = fasttext_model.wv[word]\n",
    "    \n",
    "    print(f\"Loaded {len(embeddings)} word embeddings of dimension {embedding_dim}\")\n",
    "    return embeddings\n",
    "\n",
    "def create_embedding_matrix(tokenizer, embeddings, embedding_dim=100):\n",
    "    \"\"\"\n",
    "    Create an embedding matrix for the tokenizer's vocabulary\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: Keras Tokenizer object\n",
    "        embeddings: Dictionary mapping words to embedding vectors\n",
    "        embedding_dim: Dimensionality of embeddings\n",
    "        \n",
    "    Returns:\n",
    "        Embedding matrix where row i corresponds to the embedding for word index i\n",
    "    \"\"\"\n",
    "    vocab_size = len(tokenizer.word_index) + 1  # +1 for padding token (0)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    \n",
    "    # Count words found in pre-trained embeddings\n",
    "    found = 0\n",
    "    \n",
    "    for word, idx in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "            found += 1\n",
    "    \n",
    "    print(f\"Found embeddings for {found}/{vocab_size-1} words ({found/(vocab_size-1)*100:.2f}%)\")\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: Text-to-Sequence Pipeline (B.2)\n",
    "Convert preprocessed abstracts into padded sequences of word indices aligned with the embedding matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def text_to_sequences(train_texts, dev_texts, max_words=20000, max_len=None):\n",
    "    \"\"\"\n",
    "    Convert texts to padded sequences of word indices\n",
    "    \n",
    "    Args:\n",
    "        train_texts: List of training text documents\n",
    "        dev_texts: List of development text documents\n",
    "        max_words: Maximum vocabulary size\n",
    "        max_len: Maximum sequence length (if None, computed based on data)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing tokenizer, padded sequences, and related data\n",
    "    \"\"\"\n",
    "    # Initialize and fit tokenizer on training data only\n",
    "    tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "    \n",
    "    # Convert texts to sequences\n",
    "    X_train_seq = tokenizer.texts_to_sequences(train_texts)\n",
    "    X_dev_seq = tokenizer.texts_to_sequences(dev_texts)\n",
    "    \n",
    "    # Compute sequence length statistics\n",
    "    train_lengths = [len(seq) for seq in X_train_seq]\n",
    "    \n",
    "    # Determine max_len if not specified\n",
    "    if max_len is None:\n",
    "        # Set max_len to cover 95% of documents\n",
    "        max_len = int(np.percentile(train_lengths, 95))\n",
    "    \n",
    "    # Pad sequences\n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "    X_dev_pad = pad_sequences(X_dev_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "    \n",
    "    # Visualize sequence length distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(train_lengths, bins=50)\n",
    "    plt.axvline(x=max_len, color='r', linestyle='--', label=f'Max len: {max_len}')\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Text Lengths')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "    print(f\"Training sequences: {X_train_pad.shape}\")\n",
    "    print(f\"Development sequences: {X_dev_pad.shape}\")\n",
    "    \n",
    "    return {\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"X_train_pad\": X_train_pad,\n",
    "        \"X_dev_pad\": X_dev_pad,\n",
    "        \"max_len\": max_len,\n",
    "        \"vocab_size\": len(tokenizer.word_index) + 1  # +1 for padding index 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: RNN Model Definition (B.3)\n",
    "Define the RNN model architecture using PyTorch (Simple RNN, LSTM, or GRU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, rnn_type=\"lstm\"):\n",
    "        super().__init__()\n",
    "        vocab_size, emb_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=False)\n",
    "\n",
    "        if rnn_type == \"gru\":\n",
    "            self.rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden, _) = self.rnn(x)\n",
    "        out = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)  # BiRNN\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: RNN Training Loop (B.4)\n",
    "Train the RNN on sequence data, tracking loss and accuracy across epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(model, X_train, y_train, X_dev, y_dev, epochs=10, lr=1e-3):\n",
    "    model.to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_data = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.long),\n",
    "        torch.tensor(y_train, dtype=torch.long)\n",
    "    )\n",
    "    loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Code: RNN Evaluation on Dev Set (B.5)\n",
    "Evaluate the RNN using dev data and compute relevant classification metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rnn(model, X, y, label_encoder):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(torch.tensor(X, dtype=torch.long).to(DEVICE)).argmax(dim=1).cpu().numpy()\n",
    "    print(classification_report(y, preds, target_names=label_encoder.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
