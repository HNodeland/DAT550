{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAT550 - Group Project\n",
    "### Bag of Words Document Classification using Feedforward Neural Network and Recurrent Neural Network\n",
    "Authors: Andreas etternavn, Haakon Vollheim Webb, H√•kon Nodeland, Stian etternavn\n",
    "\n",
    "\n",
    "\n",
    "The code has the following structure:\n",
    "\n",
    "1. Defining imports\n",
    "2. Reading data\n",
    "3. Cleaning data\n",
    "4. Preprocessing data\n",
    "5. Defining the model\n",
    "6. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Project 2: Bag-of-Words Document Classification with Feedforward and Recurrent Neural Networks\n",
    "\n",
    "## üéØ Objective\n",
    "Train and evaluate Feedforward Neural Networks (FFNN) and Recurrent Neural Networks (RNN) on a multiclass document classification task using various bag-of-words (BoW) feature extraction methods.\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ Dataset Information\n",
    "\n",
    "- **Source**: A subset of the ArXiv-10 dataset ([Dataset Link](https://paperswithcode.com/dataset/arxiv-10))\n",
    "- **Structure**: \n",
    "  - `abstract` (input feature)\n",
    "  - `field` (target label)\n",
    "- **Task**: Predict the field of research (10-class classification) from the article abstract.\n",
    "\n",
    "---\n",
    "\n",
    "# üó∫Ô∏è Workflow Overview\n",
    "\n",
    "Use the links below to jump directly to the relevant code sections.\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Part 0: Common Preprocessing and Setup (Shared)\n",
    "- [üß© Code: Imports and Config (0.1)](#Ô∏ècode-imports-and-config-01)\n",
    "- [üß© Code: Load and Inspect Dataset (0.2)](#code-load-and-inspect-dataset-02)\n",
    "- [üß© Code: Train-Dev Split (0.3)](#code-train-dev-split-03)\n",
    "- [üß© Code: Text Preprocessing (0.4)](#code-text-preprocessing-04)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ú® Part A: Feedforward Neural Network with Bag-of-Words (H√•kon og Haakon)\n",
    "- [üß© Code: Feature Extraction ‚Äì Bag of Words (A.1)](#code-feature-extraction--bag-of-words-a1)\n",
    "- [üß© Code: FFNN Model Definition (A.2)](#code-ffnn-model-definition-a2)\n",
    "- [üß© Code: FFNN Training Loop (A.3)](#code-ffnn-training-loop-a3)\n",
    "- [üß© Code: FFNN Evaluation on Dev Set (A.4)](#code-ffnn-evaluation-on-dev-set-a4)\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Part B: Recurrent Neural Network with Pre-trained Embeddings (Andreas og Stian)\n",
    "- [üß© Code: Load Pre-trained Word Embeddings (B.1)](#code-load-pre-trained-word-embeddings-b1)\n",
    "- [üß© Code: Text-to-Sequence Pipeline (B.2)](#code-text-to-sequence-pipeline-b2)\n",
    "- [üß© Code: RNN Model Definition (B.3)](#code-rnn-model-definition-b3)\n",
    "- [üß© Code: RNN Training Loop (B.4)](#code-rnn-training-loop-b4)\n",
    "- [üß© Code: RNN Evaluation on Dev Set (B.5)](#code-rnn-evaluation-on-dev-set-b5)\n",
    "\n",
    "---\n",
    "\n",
    "# üßπ Part 0: Common Preprocessing and Setup (Both Must Use)\n",
    "\n",
    "## üîß 0.1: Imports and Config\n",
    "\n",
    "- Load libraries (NumPy, pandas, PyTorch, scikit-learn, etc.)\n",
    "- Set random seed for reproducibility\n",
    "\n",
    "## üìÑ 0.2: Load and Inspect Dataset\n",
    "\n",
    "- Load the dataset from `.csv.gz` file\n",
    "- Print sample rows, label distribution\n",
    "\n",
    "## üß™ 0.3: Train-Dev Split\n",
    "\n",
    "- Split the data into **training** and **development** sets\n",
    "- Use **stratified splitting** to preserve label distribution\n",
    "\n",
    "### üìä Purpose of Train/Dev Split\n",
    "\n",
    "We split the dataset into **training** and **development (dev)** sets to evaluate how well the model generalizes to unseen data.\n",
    "\n",
    "- **Train Set**: Used to fit the model and update weights.\n",
    "- **Dev Set**: Used to tune hyperparameters and compare model configurations without overfitting.\n",
    "\n",
    "> ‚úÖ This separation ensures we don‚Äôt evaluate on data the model has already seen, giving us a more honest estimate of its real-world performance.\n",
    "\n",
    "\n",
    "## ‚úèÔ∏è 0.4: Text Preprocessing\n",
    "\n",
    "- Clean text (lowercasing, punctuation removal, optional stopwords)\n",
    "- Tokenization (if needed for embedding-based methods)\n",
    "\n",
    "---\n",
    "\n",
    "# ‚ú® Part A: Feedforward Neural Network with Bag-of-Words\n",
    "\n",
    "### üë§ Assigned to: **H√•kon og Haakon**\n",
    "\n",
    "## üìä A.1: Feature Extraction ‚Äì Bag of Words\n",
    "\n",
    "- Use two different vectorization techniques:\n",
    "- `CountVectorizer`\n",
    "- `TfidfVectorizer`\n",
    "- Optionally adjust:\n",
    "- `ngram_range`\n",
    "- `min_df`, `max_df`\n",
    "- `max_features`\n",
    "\n",
    "## üß† A.2: Model ‚Äì Feedforward Neural Network (MLP)\n",
    "\n",
    "- Design multiple MLP architectures with:\n",
    "- 1 hidden layer\n",
    "- 2+ hidden layers\n",
    "- Use BoW feature vectors as input\n",
    "\n",
    "## üèãÔ∏è A.3: Training\n",
    "\n",
    "- Use `CrossEntropyLoss` as the loss function\n",
    "- Optimizer: Adam or SGD\n",
    "- Track loss and accuracy per epoch\n",
    "\n",
    "## üìà A.4: Evaluation on Dev Set\n",
    "\n",
    "- Evaluate using:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- Macro-F1 Score\n",
    "- Plot confusion matrix\n",
    "- Compare models using both BoW types\n",
    "\n",
    "## üìë A.5: Summary\n",
    "\n",
    "- Compare TF-IDF vs CountVectorizer\n",
    "- Discuss impact of model depth\n",
    "- Reflect on overfitting/underfitting, training time, etc.\n",
    "\n",
    "---\n",
    "\n",
    "# üîÅ Part B: Recurrent Neural Network with Pre-trained Embeddings\n",
    "\n",
    "### üë§ Assigned to: **Andreas og Stian**\n",
    "\n",
    "## üî° B.1: Load Pre-trained Word Embeddings\n",
    "\n",
    "- Choose at least one:\n",
    "- Word2Vec\n",
    "- GloVe\n",
    "- FastText\n",
    "- Use pre-trained embeddings or train your own on external corpus (optional)\n",
    "\n",
    "## üß© B.2: Text-to-Sequence Pipeline\n",
    "\n",
    "- Tokenize each abstract into word indices\n",
    "- Convert each word to embedding\n",
    "- Pad/truncate sequences to same length\n",
    "\n",
    "## üß† B.3: RNN-Based Classifier\n",
    "\n",
    "- Use PyTorch to build models with:\n",
    "- Simple RNN\n",
    "- LSTM\n",
    "- GRU\n",
    "- Vary architectures:\n",
    "- Hidden state sizes\n",
    "- Layers\n",
    "- Bidirectional RNNs\n",
    "\n",
    "## üèãÔ∏è B.4: Training\n",
    "\n",
    "- Use `CrossEntropyLoss`\n",
    "- Optimizer: Adam\n",
    "- Monitor training loss and accuracy\n",
    "\n",
    "## üìà B.5: Evaluation on Dev Set\n",
    "\n",
    "- Metrics:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- Macro-F1 Score\n",
    "- Try different sequence pooling methods:\n",
    "- Last hidden state\n",
    "- Max/mean pooling\n",
    "- BiRNN concatenation\n",
    "\n",
    "## üìë B.6: Summary\n",
    "\n",
    "- Compare performance across embedding models and RNN types\n",
    "- Discuss tradeoffs (training time, performance, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Code: Imports and Config (0.1)\n",
    "Set up the required Python libraries and configure global settings (e.g., seed, device)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# ‚úÖ Config\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Code: Load and Inspect Dataset (0.2)\n",
    "Load the compressed dataset and inspect its structure, size, and label distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace with actual dataset path\n",
    "df = pd.read_csv(\"path/to/dataset.csv.gz\", compression=\"gzip\")\n",
    "\n",
    "# Basic inspection\n",
    "print(df.head())\n",
    "print(df[\"field\"].value_counts())\n",
    "print(f\"Dataset size: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Code: Train-Dev Split (0.3)\n",
    "Split the dataset into training and development sets using stratified sampling to preserve class balance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Stratified split on label\n",
    "train_texts, dev_texts, train_labels, dev_labels = train_test_split(\n",
    "    df[\"abstract\"], df[\"field\"], \n",
    "    test_size=0.2, \n",
    "    stratify=df[\"field\"], \n",
    "    random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Code: Text Preprocessing (0.4)\n",
    "Clean the abstract text (lowercasing, removing punctuation, etc.) and prepare it for feature extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # TODO: Expand based on dataset\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "train_texts = train_texts.apply(clean_text)\n",
    "dev_texts = dev_texts.apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Code: Feature Extraction ‚Äì Bag of Words (A.1)\n",
    "Use `CountVectorizer` and `TfidfVectorizer` to convert abstracts into numerical feature vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try both CountVectorizer and TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train = vectorizer.fit_transform(train_texts).toarray()\n",
    "X_dev = vectorizer.transform(dev_texts).toarray()\n",
    "\n",
    "# Convert labels to integers if needed\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_labels)\n",
    "y_dev = label_encoder.transform(dev_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Code: FFNN Model Definition (A.2)\n",
    "Define one or more fully connected feedforward neural networks (MLPs) using PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, num_classes):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.3))\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, num_classes))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Code: FFNN Training Loop (A.3)\n",
    "Implement the training loop for the MLP models, including loss calculation and optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ffnn(model, X_train, y_train, X_dev, y_dev, epochs=10, lr=1e-3):\n",
    "    model.to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(DEVICE)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(DEVICE)\n",
    "    X_dev_tensor = torch.tensor(X_dev, dtype=torch.float32).to(DEVICE)\n",
    "    y_dev_tensor = torch.tensor(y_dev, dtype=torch.long).to(DEVICE)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Optionally evaluate each epoch\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            dev_preds = model(X_dev_tensor).argmax(dim=1)\n",
    "            acc = (dev_preds == y_dev_tensor).float().mean().item()\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}, Dev Acc: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Code: FFNN Evaluation on Dev Set (A.4)\n",
    "Evaluate the trained MLPs on the dev set using accuracy, precision, recall, and macro-F1 score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_model(model, X, y, label_encoder):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(DEVICE)\n",
    "        preds = model(X_tensor).argmax(dim=1).cpu().numpy()\n",
    "    print(classification_report(y, preds, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Code: Load Pre-trained Word Embeddings (B.1)\n",
    "Load pre-trained embeddings (e.g., GloVe, Word2Vec) and map vocabulary to embedding vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(path=\"glove.6B.100d.txt\"):\n",
    "    embeddings = {}\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vec = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vec\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Code: Text-to-Sequence Pipeline (B.2)\n",
    "Convert preprocessed abstracts into padded sequences of word indices aligned with the embedding matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=20000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_texts)\n",
    "X_dev_seq = tokenizer.texts_to_sequences(dev_texts)\n",
    "\n",
    "max_len = 200  # or compute based on data\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
    "X_dev_pad = pad_sequences(X_dev_seq, maxlen=max_len, padding='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Code: RNN Model Definition (B.3)\n",
    "Define the RNN model architecture using PyTorch (Simple RNN, LSTM, or GRU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, rnn_type=\"lstm\"):\n",
    "        super().__init__()\n",
    "        vocab_size, emb_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=False)\n",
    "\n",
    "        if rnn_type == \"gru\":\n",
    "            self.rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden, _) = self.rnn(x)\n",
    "        out = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)  # BiRNN\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Code: RNN Training Loop (B.4)\n",
    "Train the RNN on sequence data, tracking loss and accuracy across epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(model, X_train, y_train, X_dev, y_dev, epochs=10, lr=1e-3):\n",
    "    model.to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_data = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.long),\n",
    "        torch.tensor(y_train, dtype=torch.long)\n",
    "    )\n",
    "    loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Code: RNN Evaluation on Dev Set (B.5)\n",
    "Evaluate the RNN using dev data and compute relevant classification metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rnn(model, X, y, label_encoder):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(torch.tensor(X, dtype=torch.long).to(DEVICE)).argmax(dim=1).cpu().numpy()\n",
    "    print(classification_report(y, preds, target_names=label_encoder.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
